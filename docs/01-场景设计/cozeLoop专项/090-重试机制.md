# CozeLoop 重试机制

CozeLoop 并没有把重试逻辑写死在一个地方，而是设计了 **4 层防御机制** (API -> Scheduler -> Worker -> SDK)，从用户的操作到最底层的网络请求，每一层都有明确的职责。这种 **纵深防御 (Defense in Depth)** 的设计保证了系统的健壮性。

## 1. 业务层 (API & Scheduler)

业务层的重试主要解决**逻辑错误**或**任务中断**的问题。

### 1.1 手动重试 (Manual Retry)

用户可以在界面上点击“重试”按钮。
*   **触发**: `POST /api/evaluation/v1/experiments/:expt_id/retry`
*   **机制**: 生成一个新的 RunID，仅调度那些在上一轮中失败的 Item。

### 1.2 失败重跑模式 (FailRetry Mode)

这是调度器（Scheduler）的核心能力。当实验以 `EvaluationModeFailRetry` 模式启动时，它不会盲目重跑所有数据。

*   **智能扫描**: `ExptFailRetryExec` 会扫描数据库，找出上次 **未成功 (Non-Success)** 的 Item。
*   **断点续跑**: 对于调用大模型后失败（已有 Target Result 但无 Evaluation Result）的情况，系统会复用已有的大模型结果，仅重跑评分逻辑，避免浪费 Token。
*   **参考**: [070-实验调度](070-实验调度.md#B.-失败重跑模式-`ExptFailRetryExec.ExptStart`)

---

## 2. Worker 层 (Infrastructure)

Worker 层的重试主要解决**运行时瞬时错误**（如数据库抖动、依赖服务短暂不可用）。

### 2.1 架构核心：错误处理中间件
CozeLoop 使用责任链模式 (Chain of Responsibility) 管理执行流程。重试与止损逻辑被统一收敛在 `HandleEventErr` 中间件中。

**核心代码路径**：`backend/modules/evaluation/domain/service/expt_run_item_event_impl.go`

```go
// 责任链初始化：错误处理作为最外层包裹
i.endpoints = RecordEvalChain(
    i.HandleEventErr,   // 1. [外层] 错误捕获、重试决策、债务熔断
    i.HandleEventCheck, // 2. 状态检查
    i.HandleEventLock,  // 3. 分布式锁
    i.HandleEventExec,  // 4. [内层] 核心业务逻辑
)(func(_ context.Context, _ *entity.ExptItemEvalEvent) error { return nil })
```

### 2.2 核心逻辑：HandleEventErr
该中间件通过统一的“大脑”——`Configer` 来决定错误的后继策略：是**重试**、**忽略**还是**熔断**。

```go
func (e *ExptItemEventEvalServiceImpl) HandleEventErr(next RecordEvalEndPoint) RecordEvalEndPoint {
    return func(ctx context.Context, event *entity.ExptItemEvalEvent) error {
        // 1. 执行业务逻辑，捕获错误
        nextErr := next(ctx, event)

        // 2. 【核心决策点】向 Configer 询问策略
        // Configer 根据 (错误类型, SpaceID) 返回配置：
        // - IsInDebt: 是否触发熔断（如 API Key 彻底失效）
        // - RetryTimes: 允许的最大重试次数
        // - RetryInterval: 重试等待时间
        retryConf := e.configer.GetErrRetryConf(ctx, event.SpaceID, nextErr)
        needRetry := event.RetryTimes < retryConf.GetRetryTimes()

        if nextErr == nil { return nil }

        // --- 分支 A: 债务管理 (Stop Loss) ---
        // 判定为“资不抵债”错误（如账号欠费、全局封禁），直接终结整个实验
        if retryConf.IsInDebt {
            logs.CtxWarn(ctx, "[StopLoss] Error identified as InDebt, terminating experiment...")
            e.manager.CompleteRun(ctx, event.ExptID, event.ExptRunID, ...)
            e.manager.CompleteExpt(ctx, event.ExptID, ...)
            return nil // 错误在此被消化，不再上抛
        }

        // --- 分支 B: 自动重试 (MQ Re-queue) ---
        // 判定为瞬时错误且次数未超限
        if needRetry {
            clone := event.Clone()
            clone.RetryTimes += 1
            // 重新投递到 MQ，利用 MQ 的延时投递功能实现 Backoff
            return e.publisher.PublishExptRecordEvalEvent(ctx, clone, gptr.Of(retryConf.GetRetryInterval()))
        }

        // --- 分支 C: 彻底失败 ---
        return nil 
    }
}
```

**机制总结**：Worker 层的重试是 **无状态 (Stateless)** 的。它通过将消息重新发回 MQ 来实现重试，这意味着即使当前 Worker 崩溃，消息也能被其他 Worker 消费，保证了高可用。

---

## 3. SDK 层 (LLM Runtime)

SDK 层的重试主要解决**网络层面的异常**（如 5xx 错误、超时、429 限流）。

### 3.1 配置驱动的注入 (Configuration Injection)
CozeLoop 的亮点在于：**业务代码不写死重试次数，而是由配置中心驱动底层 SDK。**

**流程**：
1.  **定义**: 在 IDL (`ProtocolConfig`) 中定义 `RetryTimes`、`Timeout` 等字段。
2.  **注入**: 在 LLM Runtime 初始化 Eino 模型时 (`backend/modules/llm/domain/service/llmimpl/eino/init.go`)，读取这些配置。
3.  **执行**: 将配置注入到具体的 Builder (如 `arkBuilder`, `qianfanBuilder`)，最终设置到底层 SDK (如 `volcengine-go-sdk`) 的 `ClientConfig` 中。

**代码示例 (Ark Builder)**：
```go
func arkBuilder(ctx context.Context, model *entity.Model, opts ...entity.Option) (einoModel.ToolCallingChatModel, error) {
    p := model.ProtocolConfig
    cfg := &ark.ChatModelConfig{ ... }
    
    // 【关键】从配置中提取重试参数，注入底层
    if arkCfg := p.ProtocolConfigArk; arkCfg != nil {
        if arkCfg.RetryTimes != nil {
            cfg.RetryTimes = ptr.Of(int(*arkCfg.RetryTimes))
        }
    }
    // Eino 内部会使用该 Config 初始化 HTTP Client 的重试策略
    return ark.NewChatModel(ctx, cfg)
}
```

**价值**：这也体现了**控制面与数据面分离**的思想。运维人员可以在不修改代码的情况下，针对不稳定的模型（如新上线的开源模型）动态调大重试次数。
