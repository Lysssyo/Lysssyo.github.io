# 01. 宏观架构：分布式协同与一致性

> **摘要**：本章从分布式系统的视角审视 Elasticsearch。我们将深入探讨其节点角色的分化设计、基于 Hash 的确定性路由算法，以及最为核心的——基于 PacificA 改进的主副本一致性模型与集群共识机制。这是理解 ES 如何在成百上千个节点间保持数据安全与服务可用的基石。

---

## 1. 逻辑架构与物理映射 (Logical vs Physical)

Elasticsearch 的核心魔力在于其抽象层。用户操作的是“索引”，而系统管理的是“分片”。

### 1.1 索引 (Index)：逻辑命名空间
从语义上讲，索引是具有相似特征的文档集合。在 7.x 废除 Type 之后，索引直接对应数据库中的“表”。
*   **物理映射**：一个逻辑索引 = $N$ 个主分片 + $N \times R$ 个副本分片。
*   **Settings**：定义了物理分布策略（`number_of_shards`, `number_of_replicas`）。

### 1.2 映射 (Mapping)：Schema 的柔性定义
Mapping 定义了文档字段的名称及其数据类型（如 `keyword`, `text`, `date`）。
*   **动态映射 (Dynamic Mapping)**：系统根据第一条写入的数据自动推断类型。
    *   *风险*：推断错误会导致后续写入失败或查询异常。生产环境建议**禁用**。
*   **显式映射 (Explicit Mapping)**：预先定义字段。
    *   *不可变性*：**字段类型一旦创建，通常不可修改**。因为倒排索引是基于特定类型和分词器生成的，修改类型意味着必须重建索引（Reindex）。

### 1.3 别名 (Alias)：零停机的逻辑指针
别名是指向一个或多个索引的“软链接”。
*   **设计思想**：应用程序永远不要直接连接具体的索引名称（如 `logs-2024-01-01`），而应连接别名（如 `logs-current`）。
*   **价值**：这是实现**零停机迁移**和**蓝绿部署**的关键。当底层索引需要重建或滚动时，只需原子性地切换别名指向，应用层无感知。

---

## 2. 集群拓扑与节点角色 (Node Roles)

Elasticsearch 是一个对等（P2P）系统，所有节点通过 Transport 层（默认 9300 端口）互联。但在逻辑层面，为了降低分布式协同的复杂度（O(N^2) 通信问题），ES 采用了严格的角色分工。

### 2.1 核心节点角色详解

#### 2.1.1 Master-eligible Node（候选主节点）
*   **职责**：负责维护集群的全局状态（Cluster State）。
    *   **Cluster State**：包含所有索引的 Mapping、Settings、分片路由表（Routing Table）、节点元数据等。这是集群的“上帝视角”。
*   **瓶颈与优化**：
    *   主节点**不参与**文档级别的索引（Index）和搜索（Search）操作。
    *   **风险点**：在一个拥有 5万+ 分片的大型集群中，Cluster State 的序列化大小可能达到 50MB+。每次集群状态变更（如分片 Rebalance），Master 需要将这 50MB 数据广播给所有节点。如果 Master 所在的网络带宽被打满，整个集群将陷入瘫痪（无响应）。
    *   **架构建议**：在 10 个节点以上的集群中，务必部署 3 个专用的（Dedicated）Master 节点（`node.master: true`, `node.data: false`），并置于独立的物理机或高优虚拟机上。

#### 2.1.2 Data Node（数据节点）
*   **职责**：持有分片（Shard）数据，执行 CRUD、搜索计算、聚合分析。
*   **资源特征**：
    *   **I/O 密集**：频繁的 Lucene Segment Merge 和 Translog 刷盘。
    *   **CPU 密集**：文本分词、脚本执行、聚合桶计算。
    *   **内存密集**：FST（倒排索引）、FieldData（如果误用）、Segment Memory 驻留。
*   **分层存储（Tiered Architecture）**：
    *   **Hot Tier**：NVMe SSD，处理当前写入和热查询。
    *   **Warm/Cold Tier**：大容量 HDD，处理时序日志的历史数据。通过 ILM（索引生命周期管理）自动迁移。

#### 2.1.3 Coordinating Node（协调节点）
*   **职责**：任何接收 REST 客户端请求的节点都自动成为该请求的“协调者”。它负责：
    1.  **请求分发（Scatter）**：解析请求，计算路由，将子请求转发给相关分片。
    2.  **结果汇聚（Gather）**：收集各分片结果，执行全局排序、聚合归并（Reduce），最终返回给客户端。
*   **深度隐患**：
    *   在执行 Deep Pagination（如查询第 1万页）或海量 Bucket 聚合时，协调节点需要将所有分片返回的数据加载到堆内存中进行归并。
    *   **OOM 风险**：如果客户端并发发送大量此类请求，协调节点极易发生 OOM（OutOfMemoryError）。
    *   **架构建议**：在查询量大的集群中，部署独立的 Coordinating-only Nodes（`node.roles: []`），作为集群的“网关”和“防火墙”，将不稳定的内存压力与数据节点隔离。

---

## 3. 数据分布与路由算法 (Routing)

分片（Shard）是数据分布的最小单元。ES 如何知道文档 `Doc-A` 存储在哪个节点？

### 3.1 确定性 Hash 路由
不同于 HDFS 使用 NameNode 记录文件块位置（元数据集中式管理），ES 使用算法实时计算位置：

$$ \text{shard\_num} = \text{hash}(\_routing) \pmod{\text{num\_primary\_shards}} $$

*   **$\_routing$**：默认是文档 ID。也可以自定义（如 UserID）。
*   **$\\text{num\_primary\_shards}$**：主分片数量。
*   **不可变性原理**：公式中的模数（主分片数）决定了计算结果。如果索引创建后修改主分片数，所有数据的 Hash 映射将全部错乱，导致数据无法找回。这就是为什么 **Reindex（重建索引）** 是修改主分片数的唯一方法。

### 3.2 自定义路由 (Custom Routing)
*   **场景**：多租户应用（SaaS），每个租户的数据独立查询。
*   **操作**：写入和查询时指定 `routing=tenant_id`。
*   **收益**：
    *   写入时：确保同一租户数据落在同一分片。
    *   查询时：协调节点直接计算 Hash，只向该分片发送请求（1个分片），而不是广播给所有分片（N个分片）。查询延迟通常降低 10倍以上。
*   **代价**：**数据倾斜（Data Skew）**。如果某个租户数据量是其他的 100 倍，该分片将成为超级热点，导致所在节点磁盘爆满。

---

## 4. 分布式一致性与共识 (Consensus & Consistency)

这是分布式系统的核心难点。ES 经历了从 Zen Discovery 到 Cluster Coordination 的重大重构。

### 4.1 脑裂防护：从 Zen 到 Raft

#### 4.1.1 Zen Discovery (ES 6.x 及之前)
*   **机制**：基于配置的多数派。
*   **核心配置**：`discovery.zen.minimum_master_nodes`。通常设置为 $N/2 + 1$。
*   **缺陷**：这是一个“被动”的检查机制。在复杂的网络分区（Partition）或节点频繁上下线场景下，仍可能因配置错误或状态不同步导致两个 Master 同时存在（双主），进而导致数据写入分裂。

#### 4.1.2 Cluster Coordination (ES 7.x 及之后)
*   **机制**：基于 **Raft** 共识算法的改进版。
*   **核心改进**：
    *   **Voting Configuration**：集群不再静态依赖配置文件中的数字，而是动态维护一份“拥有投票权”的节点列表。
    *   **Quorum**：任何集群状态的变更（提交），必须获得 Voting Configuration 中半数以上节点的确认。
    *   **Term (任期)**：引入 `cluster_state_version` 和 `term` 概念，高任期的主节点自动压制低任期的主节点。
*   **结果**：彻底解决了脑裂问题。运维不再需要手动调整 `minimum_master_nodes`。

### 4.2 数据一致性：PacificA 算法模型

ES 的数据复制（Replication）模型基于微软的 PacificA 算法，旨在保证 **强一致性（Strong Consistency）** 的配置下提供高可用性。

#### 4.2.1 核心组件
1.  **Primary Shard（主分片）**：单一的写入入口。负责将操作定序（Assign Sequence Number）。
2.  **Replica Shard（副本分片）**：被动接收主分片的操作流。
3.  **In-sync Allocation IDs（ISR 列表）**：
    *   主分片维护的一个列表，记录当前“跟得上”的副本。
    *   只有 ISR 列表中的副本，写入才会被主分片等待。
    *   如果副本宕机或网络中断，主分片向 Master 汇报，将其从 ISR 中移除。之后该副本不再阻塞写入。

#### 4.2.2 序列号与检查点 (Sequence Numbers & Checkpoints)
为了实现快速恢复和增量同步，ES 引入了严格的计数器：

*   **Sequence Number ($\_seq\_no$)**：每个写入操作（Index/Delete）分配一个递增 ID。
*   **Local Checkpoint**：
    *   每个分片（主/副）独立维护。
    *   含义：*“本分片已连续处理完所有小于等于 X 的操作”*。
    *   作用：用于本地 Translog 的垃圾回收。
*   **Global Checkpoint**：
    *   由主分片维护。
    *   公式：$Min(\text{All Active Shards' Local Checkpoints})$。
    *   含义：*“所有活跃副本都已持久化该点之前的数据”*。
    *   作用：
        *   **数据安全线**：小于等于 Global Checkpoint 的数据是绝对安全的。
        *   **快速恢复**：当副本重连时，只需从 Global Checkpoint 开始回放操作（Replay），无需全量拷贝（File-based Recovery），极大缩短了故障恢复时间。

#### 4.2.3 写入流程中的一致性保障
1.  **Request**：客户端写请求 -> 协调节点 -> 主分片。
2.  **Validate**：主分片校验请求（Mapping, Parsing）。
3.  **Primary Write**：主分片写入本地 Buffer 和 Translog，分配 `_seq_no`。
4.  **Parallel Replication**：主分片并发将操作发送给 ISR 列表中的所有副本。
5.  **Ack**：副本写入本地 Buffer 和 Translog，向主分片返回成功。
6.  **Commit**：主分片收到所有 ISR 副本的 Ack，更新 Global Checkpoint，向客户端返回成功。

> **注意**：默认配置 `wait_for_active_shards=1` 时，只需主分片写入成功即可返回。若要强一致性，需设置为 `all` 或具体数字。

---

## 5. [现代架构] 云原生存储与计算分离

### 5.1 Searchable Snapshots (可搜索快照)
*   **背景**：传统架构中，PB 级日志存储成本极高。Cold 节点即使不常被查询，也占用了昂贵的 EC2 实例和 EBS 卷。
*   **原理**：
    *   将索引的 Segment 文件完整快照到对象存储（S3/OSS）。
    *   删除本地的数据副本。
    *   **Lazy Loading**：当搜索请求到达时，ES 按需从 S3 拉取所需的索引文件块（Block），并缓存在本地（Block Cache）。
*   **性能**：由于 Lucene 文件的访问具有局部性（Locality），配合本地 SSD 缓存，查询性能仅比热节点慢 2-5 倍，但成本降低 80%。这使得 ES 能够以极低成本存储无限时长的日志数据。