---
date created: 2026-02-10 10:02:49
date modified: 2026-02-10 14:40:10
---
# MQ 可靠性机制剖析：RabbitMQ、RocketMQ 与 Kafka

## 1. 摘要

RabbitMQ、Apache RocketMQ 和 Apache Kafka三者在可靠性设计哲学上存在显著差异：

- **RabbitMQ** 依托于 AMQP 协议的严谨性，通过引入基于 Raft 共识算法的 Quorum Queues（仲裁队列），成功解决了早期镜像队列的网络分区一致性问题，成为复杂路由场景下强一致性的代表。
- **Apache RocketMQ** 秉承阿里巴巴电商业务的严苛要求，独创了基于两阶段提交（2PC）的事务消息机制，并通过同步刷盘与主从同步复制的组合，构建了金融级的消息可靠性方案。
- **Apache Kafka** 则在吞吐量与可靠性之间通过 ISR（In-Sync Replicas）机制提供了极具弹性的配置空间，利用操作系统的 Page Cache 和顺序 I/O 优势，结合幂等生产者和事务 API，实现了大规模流处理场景下的精确一次（Exactly-Once）语义。

本文将逐一拆解这三大系统的可靠性堆栈，涵盖生产端确认、服务端持久化与复制、以及消费端确认机制。

---

## 2. 分布式可靠性的理论基石

在深入具体技术实现之前，必须建立评估分布式消息系统可靠性的理论框架。可靠性并非单一指标，而是由持久性（Durability）、可用性（Availability）和一致性（Consistency）构成的三角权衡。

### 2.1 消息投递语义的严格定义

在分布式计算中，消息传递的可靠性等级通常被形式化为三种语义，每种语义都对应着不同的工程代价：

1. **最多一次（At-Most-Most）：** 消息可能会丢失，但绝不会重复。这通常被称为“即发即弃”（Fire-and-Forget）。在实现上，这意味着生产者不等待 Broker 的确认，或者消费者在处理逻辑之前就先提交了位点。虽然性能最高，但在涉及金融交易或关键审计日志的场景中是不可接受的 。
2. **至少一次（At-Least-Once）：** 消息绝不会丢失，但可能会重复。这是大多数分布式消息系统的基准要求。它要求生产者在未收到 Broker 确认时无限重试，且消费者仅在业务逻辑执行成功后才提交确认。这种机制将去重的复杂性转嫁给了应用层，要求下游业务具备幂等性（Idempotency）。
3. **精确一次（Exactly-Once）：** 消息不仅不会丢失，也不会重复，且对系统状态的影响就像只处理了一次一样。这是最理想但也最难实现的语义。它通常需要消息系统与流处理引擎或外部存储系统的深度协同，例如 Kafka 的事务 API 结合 `read_committed` 隔离级别 。

### 2.2 CAP 定理在消息队列中的投影

> [!TIP] CAP核心
> 当P发生，必须在AP和CP之间做出选择

CAP 定理指出，分布式数据存储最多只能同时满足一致性（C）、可用性（A）和分区容错性（P）中的两项。对于跨网络部署的消息队列，P 是必须面对的现实（网络分区随时可能发生），因此设计者必须在 C 和 A 之间做出选择：

- **CP 倾向（RabbitMQ Quorum Queues, RocketMQ DLedger）：** 当发生网络分区或主节点宕机时，系统会暂停写入服务，直到选出新的 Leader 并完成日志同步。这种设计优先保证数据不丢失、不分叉（Split-Brain），但在选举期间会出现短暂的不可用。
- **AP 倾向（Kafka default, RabbitMQ Legacy Mirrored Queues）：** 为了保证写入的高可用，允许数据在未完全同步到所有副本的情况下被确认，或者允许非同步副本（Non-ISR）参与选举。这虽然保证了服务的连续性，但在故障恢复后可能面临数据丢失或回滚的风险 。

> [!tip] 网络分区
> **网络分区 (P - Partition Tolerance)** 指的是：由于网络故障（如光缆中断、路由器崩溃、极高的延迟），分布式系统中的节点被切割成了两个或多个互不连通的区域。
> 在这种情况下，虽然所有的节点（服务器）本身可能仍在正常运行，但它们之间**无法传递消息**。系统被分裂成了几个孤立的“孤岛”。

### 2.3 可靠性的一致性模型

消息系统的可靠性链条由三个环节组成：**生产端可靠性**（确保消息进入 Broker）、**服务端可靠性**（确保消息持久化并复制）、**消费端可靠性**（确保消息被正确处理）。任何一个环节的断裂都会导致数据丢失。

---

## 3. RabbitMQ：从协议规范到共识算法的演进

### 3.1 生产端可靠性：确认机制与事务

在 RabbitMQ 中，生产者将消息发送到交换机（Exchange），再由交换机路由到队列。为了确保消息成功到达并被安全处理，RabbitMQ 提供了两种机制：**AMQP 事务** 和 **发布者确认（Publisher Confirms）**。

#### 3.1.1 AMQP 事务的局限性

AMQP 协议原生支持事务原语（`tx.select`, `tx.commit`, `tx.rollback`）。在事务模式下，Broker 只有在收到 `tx.commit` 指令后，才会将消息真正投递到队列并持久化。

- **原子性保证**：事务确保了多条消息的发送要么全部成功，要么全部失败。
- **性能瓶颈**：由于事务是**阻塞式**的，涉及大量磁盘同步（fsync），吞吐量极低。基准测试表明，开启事务后吞吐量可能下降 **250 倍**以上。

> [!CAUTION] 结论
> 在追求高并发的现代架构中，**AMQP 事务已极少被使用**。

#### 3.1.2 发布者确认—— 事实标准

为了解决事务的性能问题，RabbitMQ 引入了轻量级的发布者确认机制。

- **异步流转**：生产者通过 `confirm.select` 开启确认模式。Broker 为信道上的每条消息分配一个单调递增的 `Delivery Tag`。
- **确认时机**：
  - **非持久化消息**：Broker 在消息被路由到所有匹配队列后立即发送 `basic.ack`。
  - **持久化消息**：Broker 只有在消息被写入磁盘（fsync）之后，或者在 Quorum Queues 中被复制到 Raft 集群的大多数节点（Majority）之后，才会发送 `basic.ack`。
- **否定确认 (Nack)**：如果 Broker 因为内部错误（如磁盘满、进程崩溃）无法处理消息，会发送 `basic.nack`，为生产者提供明确的重试信号。
- **背压机制 (Back Pressure)**：发布者确认不仅仅是可靠性机制，也是流控机制。如果 Broker 负载过高，它会推迟发送确认，迫使生产者降低发送速率，从而保护 Broker 不被压垮 。

> [!IMPORTANT] 核心洞察
> **在使用 Quorum Queues 时，发布者确认是客户端与共识系统交互的唯一桥梁**。只有收到 Confirm，客户端才能确信消息不仅到达了 Leader，而且已经被安全地复制到了 Follower 节点，具备了容灾能力。

### 3.2 服务端可靠性：仲裁队列与 Raft

RabbitMQ 4.0 版本正式移除了传统的经典镜像队列（Classic Mirrored Queues），确立了 Quorum Queues 为高可靠场景的唯一选择。理解这一变革对于掌握 RabbitMQ 的可靠性至关重要。

#### 3.2.1 经典镜像队列的缺陷

早期的镜像队列采用链式复制算法（Guaranteed Multicast），存在以下核心痛点：

1. **O(N) 延迟**：消息必须沿着镜像节点环走完一圈，Master 收到回执后才向生产者发 Ack。**集群规模越大，写入性能越差**。
2. **木桶效应**：环中任何一个节点磁盘变慢或负载过高，都会阻塞整个集群的流转。
3. **同步阻塞**：新节点加入时必须同步所有历史消息，期间整个队列往往处于不可用状态。
4. **脑裂风险**：在网络分区场景下，镜像队列的处理逻辑（如 `pause_minority`）较为脆弱，容易出现数据丢失或脏读。

#### 3.2.2 Quorum Queues 的 Raft 实现

Quorum Queues 基于 Raft 共识算法实现，彻底改变了数据复制方式。

> 参考：[Raft 一致性协议](../../05-分布式/020-Raft%20一致性协议.md)

- **多数派写入：** 一条消息被认为是“已提交”的，当且仅当它被写入了 Leader 的日志，并被复制到了集群中 `(N/2)+1` 个节点的日志中 。
- **日志复制与快照：** Quorum Queue 的核心是一个仅追加（Append-Only）的日志文件。所有的队列操作（入队、出队、Ack）都是日志条目。为了防止日志无限增长，RabbitMQ 会定期执行快照（Snapshot）并压缩日志。
- **总是持久化：** 不同于经典队列可选 transient（内存）模式，Quorum Queues 设计之初就是为了数据安全，因此强制要求消息持久化。内存中只保留热数据，冷数据会迅速落盘。
- **Leader 选举：** 当 Leader 节点故障时，Raft 算法能保证选出的新 Leader 一定拥有所有已提交的日志条目。这意味着在故障转移过程中，已确认的消息绝对不会丢失。

##### 1. “写完不丢”保障

> 对应机制：**多数派写入 + 强制持久化**

在传统的镜像队列中，主节点收到消息写进内存，可能还没来得及同步给从节点，主节点就宕机了，导致消息永久丢失。但在 Quorum Queues 下：
- **动作**：客户端发送消息 `Msg A`。
- **过程**：Leader 必须将 `Msg A` 写入本地磁盘（WAL），并等待集群中**大多数节点**（如 3 节点中的 2 个）也同步写盘成功。
- **结果**：只有达成“多数派共识”后，Leader 才会向客户端返回 `Ack`。这确保了即使 Leader 立即宕机，数据也已安全存储在其他节点中。

##### 2. “日志完整性”保障 

> 对应机制：**确定性选主**

这是 Raft 协议在处理故障转移时的关键逻辑，优于传统的主从同步。

- **场景**：集群 A, B, C 中，Leader A 已同步 `Msg 100` 给 B 但未给 C，随后 A 宕机。
- **冲突**：若 C 因网络响应快而尝试竞选 Leader，由于它的日志版本较旧（缺少 `Msg 100`），B 会根据 Raft 铁律 **“选票只投给日志比我新（或一样新）的人”** 拒绝给 C 投票。
- **结论**：最终只有拥有最新提交数据（`Msg 100`）的 B 能够当选。这保证了新 Leader 始终包含所有已确认的消息，历史记录绝不会被“篡改”或丢失。
- **传统队列的隐患**： 如果 C 被选为新 Leader（因为它网络好），它根本就没有 `Msg 100`。客户端之前明明收到了 `Ack`，现在连上去一查，`Msg 100` 没了！这就是**数据丢失**。

##### 3. “死而复生不错乱”保障

> 对应机制：**日志复制与快照**

当故障节点（如旧 Leader A）修复并重新加入集群时，它可能存在过时的脏数据或缺失新日志。

- **日志对齐**：A 必须无条件服从当前新 Leader 的指令。
- **截断与追加**：新 Leader 会指示 A 截断（Truncate）与集群不一致的脏日志，并从冲突点开始追加最新的同步日志。
- **快照 (Snapshot)**：若节点落后太多，Leader 会直接发送“状态快照”（压缩后的当前全量数据），让节点实现瞬间对齐。

#### 3.2.3 毒丸消息（Poison Message）处理

Quorum Queues 引入了对“毒丸消息”的原生支持。如果一条消息导致消费者反复崩溃（例如格式错误导致解析异常，消费者尝试反序列化或处理业务逻辑，程序抛出异常，**连接断开** 或 **显式 Nack(requeue=true)**），传统的队列可能会陷入死循环。Quorum Queues 跟踪每个消息的投递尝试次数（`x-delivery-count`），一旦超过阈值，自动将消息投递到死信交换机（DLX），从而保护队列的整体可用性 。

### 3.3 消费端可靠性：手动确认与 QoS

**RabbitMQ 采用推模式（Push）将消息投递给消费者**，其可靠性依赖于显式的确认机制。

#### 3.3.1 自动确认 vs. 手动确认

| 确认模式                  | 消息删除时机            | 优点             | 风险点                                |
| :-------------------- | :---------------- | :------------- | :--------------------------------- |
| **自动确认 (Auto-Ack)**   | 写入 TCP 发送缓冲区后立即删除 | 吞吐量极高          | 网络丢包、消费者缓存堆积崩溃、业务报错均会导致**消息永久丢失**。 |
| **手动确认 (Manual Ack)** | 收到消费者显式 Ack 信号后删除 | **高可靠**，支持重回队列 | 处理逻辑不当可能导致消息积压或内存溢出。               |
在手动确认模式下，RabbitMQ 将队列中的消息分为两个状态：

- **Ready**：等待投递给消费者的消息。
- **Unacked**：已经投递给消费者，但尚未收到确认的消息。

**注意：这部分消息不会被删除，只是暂时不可见。** 消费者必须显式调用 `channel.basicAck(deliveryTag, false)`。只有收到这个信号，Broker 才会真正删除消息。

> [!TIP] 异常流程：消息重回队列 (Re-queue)
> RabbitMQ 不使用超时机制，而是通过 **TCP 连接状态** 判断。如果消费者处理中途崩溃导致 TCP 断开，Broker 会感知到 Socket 关闭，立即将该消费者持有的 `Unacked` 消息重新变回 `Ready` 并投递给其他消费者。

**消费手动确认动作对比：**

| **动作** | **协议支持** | **批量操作 (multiple)** | **重回队列 (requeue)** | **典型应用场景** |
| :--- | :--- | :--- | :--- | :--- |
| **Ack** | AMQP 标准 | 支持 | N/A | 业务处理成功。 |
| **Reject** | AMQP 标准 | **不支持** | 可选 | 拒绝单条非法消息。 |
| **Nack** | **RabbitMQ 扩展** | **支持** | 可选 | 严重故障时批量拒绝消息。 |

- **首选 `Nack`**：在 RabbitMQ 中，尽量使用 `basic.nack`，因为它功能涵盖了 `reject` 且更灵活（支持批量）。
- **慎用 `requeue=true`**：
    - 如果是因为**代码逻辑错误**（如空指针异常）导致的失败，无限 `requeue=true` 会导致死循环（消息一直被投递、报错、投递、报错），也就是常说的“**毒消息**” (Poison Message)。
    - **正确做法**：对于处理不了的异常消息（ `reject` 或者 `nack` ），使用 `requeue=false` 并配置 **死信队列 (DLX)**，让消息去那里“冷静一下”，等待人工修复。

#### 3.3.2 服务质量（QoS）与预取（Prefetch）

在使用**手动确认**时，必须设置 `basic.qos(prefetch_count = N)`：

- **Prefetch Count：** 限制了 Broker 在收到消费者确认之前，可以向其投递的最大未确认消息数。
- **可靠性影响：** 如果设置得太高，消费者崩溃时会导致大量已预取但未处理的消息需要重新入队，增加系统负担；如果设置得太低（如 1），则会因为网络往返（RTT）导致吞吐量大幅下降。合理的 QoS 设置是平衡性能与故障恢复成本的关键。

> [!TIP]
> 手动确认保证的是 **At-Least-Once (至少一次)** 投递，而不是 Exactly-Once。请务必结合业务幂等性。

#### 3.3.3 至少一次的死信处理

在 Quorum Queues 中，死信处理（Dead Lettering）采用了更严格的机制：
- 传统的死信处理是“最多一次”的，即消息在转移过程中可能丢失。
- Quorum Queues 的内部死信消费者使用**发布者确认机制**，只有收到 DLX 的成功确认后，原队列才会删除消息，实现了“**至少一次**”的死信流转。

---

## 4. Apache RocketMQ：金融级可靠性的深度解构

Apache RocketMQ 是阿里巴巴为应对“双十一”流量洪峰而自研的消息中间件，其设计目标直指金融级的高可靠性、低延迟和强一致性。RocketMQ 在架构上采用了存储与计算分离、主从同步复制等机制，并独创了事务消息功能。

### 4.1 生产端可靠性：多重发送与事务保障

RocketMQ 的生产者客户端（Producer SDK）内置了丰富的容错逻辑，提供了同步、异步和单向三种发送模式。

#### 4.1.1 发送模式的权衡

[Producer：消息生产者](020%20-%20Apache%20RocketMQ%20深度剖析.md#2.3%20Producer：消息生产者)

#### 4.1.2 事务消息：解决“双写”一致性难题

[事务消息 (Transactional Messages)](020%20-%20Apache%20RocketMQ%20深度剖析.md#5.1%20事务消息%20(Transactional%20Messages))

这种机制保证了消息系统与业务数据库的**最终一致性**，完美解决了分布式事务中的“悬挂事务”问题。

### 4.2 服务端可靠性：刷盘策略与复制架构

RocketMQ 的存储层设计直接决定了其数据的持久性。

#### 4.2.1 存储架构：CommitLog 与 ConsumeQueue

RocketMQ 采用混合型存储结构。**所有 Topic 的消息都顺序写入同一个物理文件 `CommitLog`。这利用了磁盘的顺序写性能，吞吐量极高**。为了支持消息检索，Broker 会异步构建 `ConsumeQueue`（逻辑索引）和 `IndexFile`（哈希索引）。

 **可靠性隐患与对策：** 理论上，如果 CommitLog 写入成功但索引构建失败，消息是否存在？RocketMQ 的设计是，只要 CommitLog 落盘，消息就算安全。Broker 重启时会重放 CommitLog 来重建索引，确保数据完整性。

#### 4.2.2 刷盘策略（Flush Policy）

[刷盘策略 (Flush Policy)](020%20-%20Apache%20RocketMQ%20深度剖析.md#3.3%20刷盘策略%20(Flush%20Policy))

#### 4.2.3 高可用一致性架构

[高可用与一致性架构演进](020%20-%20Apache%20RocketMQ%20深度剖析.md#4.%20高可用与一致性架构演进)

### 4.3 消费端可靠性：重试队列与位点管理

RocketMQ 的消费端可靠性依赖于严格的 Offset 管理和重试机制。

#### 4.3.1 消费重试与死信

当 Consumer 消费失败时，它实际上是向 Broker 发送了一个 ACK，告知该消息需要重试。如果 Consumer 挂掉或网络断开导致没有发送 ACK，Broker 会在超时后认为该消息未被消费，也会触发重试（但这种通常是从本地缓存或再次拉取）。

- **阶梯重试：** 消息会被投递到特殊的 `%RETRY%` 队列。Broker 会按照特定的时间间隔（1s, 5s, 10s, 30s... 2h）逐步增加重试延时 。
- **死信队列（DLQ）：** 当重试次数超过上限（默认 16 次）后，消息被移入死信队列，需人工干预。

#### 4.3.2 消费位点（Offset）持久化

- **集群模式（Clustering）：** Offset 由 Broker 管理。消费者定期将处理进度提交给 Broker。如果消费者崩溃，新的消费者从 Broker 获取最后提交的 Offset 继续消费。这保证了“至少一次”消费。
- **广播模式（Broadcasting）：** Offset 由消费者本地管理。

---

## 5. Apache Kafka：流式架构下的弹性可靠性

Apache Kafka 的设计哲学与前两者截然不同。它将自己定义为“分布式流平台”，其可靠性构建在类似于分布式文件系统的日志复制机制之上。Kafka 并不试图在每一条消息上都做“强一致性”的同步确认，而是通过灵活的配置（In-Sync Replicas）在吞吐量和持久性之间寻找平衡。

### 5.1 生产端可靠性：Acks、ISR 与 幂等性

Kafka 生产者的可靠性完全由 `acks` 参数控制，这给了用户极大的自由度。

#### 5.1.1 Acks 配置详解

- `acks=0`：生产者不等待任何响应。吞吐量极高，但网络抖动即丢数据。
- `acks=1`：Leader 写入本地日志即返回成功。
    - _风险：_ 如果 Leader 刚写入完就宕机，且数据尚未复制到 Follower，则数据永久丢失。
- `acks=all` (或 `-1`)：Leader 等待所有**同步副本（ISR）** 都确认收到消息后，才返回成功。这是 Kafka 实现零数据丢失的基础 。
    

#### 5.1.2 最小同步副本

仅设置 `acks=all` 是不够的。如果 ISR 集合中只剩下 Leader 一个节点（其他 Follower 都掉线了），`acks=all` 实际上就退化成了 `acks=1`。

为了防止这种情况，必须在 Broker 端配置 `min.insync.replicas`（最小同步副本）。

- **示例场景：** 假设 `replication.factor=3`，`min.insync.replicas=2`。
- **行为：** 如果 ISR 数量小于 2，Broker 会直接拒绝生产者的写入请求（抛出 `NotEnoughReplicasException`）。
- **CAP 权衡：** 这是一种典型的 CP 行为。系统宁愿牺牲可用性（无法写入），也要保证一致性（防止数据单点存储）。
    

#### 5.1.3 幂等生产者

为了实现**分区内的精确一次**写入，Kafka 引入了幂等性（`enable.idempotence=true`）。

- **PID 与 序列号：** 生产者启动时会被分配一个 Producer ID (PID)。发送到每个分区的每批消息都带有单调递增的序列号。
- **去重逻辑：** Broker 在内存和日志中维护每个 PID 的最后序列号。如果收到一个序列号小于等于当前值的消息，Broker 会直接丢弃并返回 Ack，防止因网络重试导致的重复写入。
- **生命周期：** 这种保证仅限于单个生产者会话。如果生产者重启，PID 会变化，幂等性重置 。
    

### 5.2 服务端可靠性：日志复制哲学

#### 5.2.1 ISR 机制与高水位

Kafka 的复制算法既不是同步复制（太慢），也不是纯异步复制（不安全），而是基于 ISR 的动态复制。

- **LEO (Log End Offset)：** 副本当前写入的最高位点。
- **HW (High Water Mark)：** ISR 集合中所有副本共享的最小 LEO。
- **消费者可见性：** 只有 Offset < HW 的消息才对消费者可见。这保证了消费者读到的数据是已经达成共识的，即使 Leader 切换，这部分数据也不会丢失 。

## 5.3 消费者端的可靠性

**Kafka 原生（Broker 端）完全没有类似于 RocketMQ 失败重试或者死信队列的机制**。

RocketMQ 的重试和死信机制是其 Broker 端的核心特性，而 Kafka 的设计哲学是“**Log 原语**”，它只负责高性能的消息追加和读取。Kafka Broker 不关心消息是否被消费成功，也不支持“单条消息的延迟投递”。

---

## 6. 综合对比与场景化分析

为了更直观地展示三者的差异，本节通过对比表格和典型故障场景进行分析。

### 6.1 核心可靠性指标对比表

| **特性维度**   | **RabbitMQ (Quorum)** | **RocketMQ (Sync/DLedger)**      | **Kafka (Standard)**       |
| ---------- | --------------------- | -------------------------------- | -------------------------- |
| **一致性算法**  | **Raft** (强一致)        | **Raft** (DLedger) / 主从同步        | **ISR** (动态同步集合)           |
| **单机持久化**  | **WAL + Fsync** (批处理) | **Sync Flush** (强) / Async Flush | **OS Page Cache** (弱，依赖复制) |
| **数据重复**   | 极低 (Raft 保证)          | 可能 (At-Least-Once)               | 无 (幂等性开启时)                 |
| **事务支持**   | AMQP 事务 (极慢)          | **2PC 事务消息** (最终一致性)             | **流式事务** (Exactly-Once)    |
| **网络分区行为** | **CP** (暂停写入，保数据)     | **CP** (DLedger 模式)              | **可配置** (默认 CP，可降级为 AP)    |
| **消费模型**   | Push (预取控制)           | Pull (长轮询)                       | Pull (批量拉取)                |
| **最大优势**   | 复杂路由下的数据安全            | 业务与消息的事务一致性                      | 海量吞吐下的可配置可靠性               |

### 6.2 典型故障场景演练

#### 场景一：Broker 所在服务器突然掉电

- **RabbitMQ:** 如果使用 Quorum Queues，且 Producer 收到了 Confirm，数据已在多数节点落盘，**零丢失**。
- **RocketMQ:**
    - `SYNC_FLUSH`: **零丢失**。
    - `ASYNC_FLUSH`: 丢失约 0.5s 数据（内存中未刷盘部分）。
- **Kafka:**
    - `acks=all`: 只要其他副本（ISR）存活，**零丢失**。
    - 如果所有副本同时掉电（极罕见），则会丢失 Page Cache 中的数据。
        

#### 场景二：消费者处理缓慢导致积压

- **RabbitMQ:** 性能会显著下降。虽然 Quorum Queues 将消息落盘，但大量堆积会消耗内存（索引）和 CPU。不建议堆积超过数百万条。
- **RocketMQ:** 极强。CommitLog 顺序写与消费逻辑解耦，堆积十亿条消息通常不影响写入性能。
- **Kafka:** 极强。基于文件的存储，堆积本质上只是磁盘占用，对读写性能几乎无影响。
    

#### 场景三：网络分区（Split-Brain）

- **RabbitMQ:** Raft 机制会自动暂停少数派分区的写入。分区恢复后，少数派节点会作为 Follower 重新加入并截断冲突日志。
- **RocketMQ (DLedger):** 类似 Raft，少数派无法选主，写入失败。保证一致性。
- **Kafka:** 取决于 `min.insync.replicas`。如果设置为 >1，少数派分区会拒绝写入。如果为 1，可能会发生数据分叉，且在恢复时旧 Leader 的数据可能被截断（Data Loss）。

---

## 7. 结论与选型建议

分布式消息系统的可靠性不是一个简单的“开/关”选项，而是一个由业务需求驱动的架构决策。

- **选择 RabbitMQ 的场景：** 如果您的业务逻辑复杂，需要灵活的路由（Topic/Fanout/Header Exchange），且消息量级适中（每秒几万到十万级），但对每条消息的**状态一致性要求极高**（如订单状态流转），RabbitMQ 的 Quorum Queues 是最稳妥的选择。它将复杂的共识算法封装得最好，对业务侵入最小。
- **选择 Apache RocketMQ 的场景：** 如果您身处**金融、支付、电商交易**核心链路，不仅要求消息不丢，还要求**数据库事务与消息发送的强一致性**，RocketMQ 的事务消息是业界目前唯一的标准解法。其同步刷盘机制虽然牺牲了部分性能，但提供了最底层的物理数据安全感。
- **选择 Apache Kafka 的场景：** 如果您的目标是处理**海量日志、用户行为数据、实时流计算**，且吞吐量要求达到百万级 TPS，Kafka 是不二之选。通过合理配置 `acks=all`、`min.insync.replicas=2` 和 `replication.factor=3`，Kafka 可以在提供极高性能的同时，达到 99.999% 以上的数据可靠性。
    

综上所述，没有任何一个系统能“开箱即用”地保证 100% 可靠。架构师必须深入理解上述的 Confirm、Fsync、Raft、ISR 等机制，结合具体的业务容忍度（RPO/RTO），进行精细化的参数调优，才能构建出真正坚不可摧的消息基础设施。