# Elasticsearch 分布式协同与一致性机制

本文深入解析 Elasticsearch 的分布式架构原理，涵盖节点角色、集群协同、数据持久化、并发控制及故障恢复等核心机制。

## 1. 节点角色与存储模型

ES 的分布式架构天然支持水平扩展，通过将不同的职责分配给不同的节点角色来保证系统的高可用性与高性能。

### 1.1 分布式节点角色

#### 1.1.1 主节点（Master Node）

主节点是集群的“大脑”，负责集群层面的轻量级管理工作。其核心职责包括**维护和更新集群状态**（Cluster State），这是一份包含所有索引元数据、分片路由表、节点信息等关键数据的全局视图。主节点必须负责创建或删除索引、跟踪哪些节点是集群的一部分，以及决定将分片分配给哪些数据节点。

在 ES 的设计中，主节点的稳定性对集群至关重要。如果主节点过载或网络不稳定，可能导致集群状态更新受阻，进而引发“脑裂”或集群不可用。

**最佳实践：专用主节点 (Dedicated Master)**

在生产环境中，随着集群规模的增长，混合角色节点的“全能模式”会成为瓶颈。通常建议配置独立的、不处理数据的专用主节点（Dedicated Master-eligible Node），以隔离数据处理带来的 CPU 和内存压力。通常配置 3 个小规格实例，仅仅负责集群状态维护，避免因为 Data 节点的 GC 停顿或高负载导致选主超时。

#### 1.1.2 数据节点（Data Node）

数据节点是集群中负荷最重的角色。它们**持有数据分片（Shard）**，执行数据相关的操作，如文档的 CRUD（增删改查）、搜索倒排索引和执行聚合分析。数据节点是 I/O 密集型、内存密集型和 CPU 密集型的，其物理资源的配置（如磁盘类型、堆内存大小）直接决定了读写的吞吐量与延迟。数据节点通常分为不同的层级（Tiers），如热节点（Hot）、温节点（Warm）和冷节点（Cold），以适应不同生命周期数据的存储需求。

#### 1.1.3 协调节点（Coordinating Node）

从概念上讲，协调节点并非一个静态的角色配置，而是一个动态的请求处理状态。**任何接收到客户端 RESTful 请求的节点都会自动成为该请求的“协调节点”**。它负责解析请求、根据路由规则将请求分发（Scatter）到持有相关数据的分片（对于写操作是主分片，对于读操作是主分片或副本分片），并将各分片返回的最终结果汇集（Gather）后返回给客户端。

**最佳实践：独立协调节点 (Coordinating-only Node)**

在处理大规模搜索请求时，协调节点在 Scatter-Gather 阶段（尤其是 Fetch Phase）需要缓存和排序大量的临时结果，因此会消耗大量堆内存。如果协调节点同时也是数据节点，繁重的搜索聚合任务可能导致节点 OOM（Out of Memory）并影响数据写入。因此，在大型集群中（特别是涉及深度聚合 Deep Aggregation 场景），部署独立的协调节点（Client Node）是一种常见的架构优化手段。

#### 1.1.4 仅投票节点（Voting-only Node）

在双机房（2 Zones）场景下，我们需要第 3 个节点来打破投票平局。但我们可能不想为此部署一个完整的 Master。Voting-only 节点只参与投票，不当选，不存数据，成本极低，是架构设计中的“神来之笔”。

### 1.2 数据分片与 Lucene 索引

ES 的核心抽象是“索引（Index）”，它是一个逻辑命名空间。而在物理层面，ES 通过分片（Sharding）机制实现了数据的分布式存储。

#### 1.2.1 分片即 Lucene 索引

理解 ES 读写性能的关键在于认识到：**一个 ES 分片本质上就是一个独立的、功能完整的 Lucene 索引实例**。Lucene 是 Java 编写的高性能信息检索库，它管理着倒排索引（Inverted Index）、词典（Term Dictionary）、行存数据（Stored Fields）和列存数据（Doc Values）。

当我们在 ES 层面谈论“写入一个文档”时，实际上是在向底层的 Lucene 实例添加数据。Lucene 的设计决定了 ES 的许多特性，例如 Segment（段）的不可变性决定了 Update 操作实际上是“标记删除 + 新增”，以及 NRT 近实时搜索依赖于 Segment 的刷新机制。

#### 1.2.2 主分片与副本分片

ES 采用主从复制模型（Primary-Backup Model）来保证数据的高可用性。

- **主分片（Primary Shard）**：数据的“权威”副本。所有的写操作（索引、更新、删除）必须首先在主分片上执行成功，然后才会并行复制到副本分片。主分片的数量在索引创建时定义，且后续难以修改（除非使用 Split/Shrink API 进行重索引），这直接关系到数据的路由算法。
- **副本分片（Replica Shard）**：主分片的完整拷贝。它们主要有两个作用：一是提供高可用性（Failover），当主分片所在节点宕机时，副本可以被迅速提升为主分片；二是提升读性能，搜索请求可以在主副分片之间负载均衡，增加副本数量可以直接提升系统的读取吞吐量（Read Throughput）。

---

## 2. 分布式协同与选主机制

主节点是 Elasticsearch 集群的大脑，负责维护全局唯一的“集群状态”（Cluster State）。集群状态包含了所有节点的元数据、索引的 Mapping 与 Setting、分片的路由表（Routing Table）等关键信息。

### 2.1 演进：从 Zen Discovery 到确定性共识

#### 2.1.1 早期架构的困境 (Pre-7.0)

在 Elasticsearch 7.0 之前，集群协同依赖于一个名为 Zen Discovery 的内置模块。

*   **静态法定人数与脑裂风险**：早期的 Zen Discovery 依赖于用户手动配置 `discovery.zen.minimum_master_nodes` 参数（通常建议设置为 `N/2 + 1`）。这是一个静态配置，当集群进行扩缩容时，管理员必须极其谨慎地同步更新此参数。如果忘记更新，集群可能面临“脑裂”（Split-Brain）风险，即分裂成两个独立的集群同时接受写入，或者因为无法满足法定人数而停止服务。
*   **选主算法的非确定性**：Zen Discovery 的选主过程基于 Gossip 协议，缺乏严格的轮次（Term）概念。在网络不稳定的情况下，节点可能陷入“反复选举”的死循环，甚至出现极罕见的“状态更新丢失”问题。

#### 2.1.2 现代协调子系统 (7.0+)

Elasticsearch 7.0 引入了重写的集群协调子系统。这一系统的设计并不直接照搬 Raft 或 Paxos，而是针对 Elasticsearch 的特性进行了定制，并使用了 **TLA+（Temporal Logic of Actions）** 进行了严格的形式化验证。
*   **原子寄存器与共识**：新系统的核心模型是将集群状态视为一个“单一的、可重写的寄存器”。任何对该寄存器的写入（即状态更新）都必须经过一个类似 Paxos 的两阶段提交过程。
*   **安全性保证**：TLA+ 模型检查器遍历了所有可能的执行路径，验证了算法的安全性（不会发生两个 Master 同时提交不同状态）和活性（最终一定能选出 Master）。

### 2.2 深入剖析现代选主与状态更新

#### 2.2.1 动态投票配置 (Voting Configuration)

ES 摒弃了静态的 `minimum_master_nodes`，现在自动维护一个名为“投票配置”的节点集合。

*   **机制**：这是一个由 Master-eligible 节点组成的集合。任何决策（选主或状态提交）必须获得该集合中“过半数”节点的认可。
*   **动态调整**：当节点加入或离开集群时，Master 节点会自动发起一项特殊的集群状态更新“重配置”（Reconfiguration）来修改这个集合。
*   **运维警示**：**禁止同时下线过半节点**。如果你有一个 5 节点的集群，你绝对不能同时停止 3 个节点。因为剩下的 2 个节点无法凑齐 3 票，无法完成任何状态更新，集群将进入 Block 状态。此时唯一的恢复手段是使用 `elasticsearch-node unsafe-bootstrap` 工具，但这可能会导致数据丢失。

#### 2.2.2 逻辑时钟与预投票 (Pre-voting)

新的选主算法引入了类似于 Raft 的机制。

*   **Term (任期)**：每个节点都维护一个单调递增的整数 `current_term`。每当选主发生时，Term 增加。节点拒绝任何来自旧 Term 的 Master 的请求。这天然地解决了“僵尸 Master”问题。
*   **Pre-voting 机制**：在发起正式选举之前，候选节点（Candidate）会先发起一轮“预投票”，询问其他节点“如果我发起选举，你们会投我吗？”。只有收集到过半数的预投票赞成票（即确认当前 Master 确实失效），Candidate 才会正式增加 Term 并发起选举。这避免了网络抖动导致的 Term 恶性膨胀和频繁 Leader 切换。

#### 2.2.3 集群状态发布的二阶段提交协议

一旦 Master 当选，它必须确保集群状态的更新是原子的。Elasticsearch 采用了一种改良的二阶段提交（2PC）协议。

1.  **Publish (发布)**：Master 计算出状态增量（Diff），广播给所有节点。接收节点校验 Term 和有效性后，将 Diff **暂存**在内存中（并不应用），并返回 ACK。
2.  **Commit & Apply**：Master 等待收到投票配置中“过半数”节点的 ACK（Quorum Check）。一旦满足，Master 认为状态已提交（Committed），并广播 Apply 指令。接收节点正式应用新状态。Master 收到所有节点的 Apply 确认后，流程结束。

### 2.3 脑裂防御最佳实践

*   **奇数节点原则**：始终保持 Master-eligible 节点为奇数（3, 5, 7）。
*   **Zone Awareness**：利用 `cluster.routing.allocation.awareness.attributes` 将分片强制分布在不同机架或机房。结合新版协调系统的自动配置更新，即便整个机房断电，只要剩下的节点满足 Quorum，集群即可自动恢复。

---

## 3. 数据一致性、并发控制与故障恢复

在分布式环境下，Elasticsearch 如何保证数据不乱序、不覆盖，并在故障时快速恢复？

### 3.1 检查点机制：Sequence IDs, LCP 与 GCP

为了实现快速恢复和精确的差异同步，ES 引入了几个关键指针：

*   **序列号 (Sequence Number)**：每个操作都被分配一个递增的整数 `_seq_no`。`(_primary_term, _seq_no)` 唯一标识了集群历史中的一个操作。
*   **Local Checkpoint (LCP)**：每个分片（主或副）本地维护的一个序列号，表示在这个序列号之前的所有操作都已处理完毕且连续（没有空洞）。
*   **Global Checkpoint (GCP)**：由主分片维护，是所有 **In-Sync** 副本的 LCP 的最小值。
    *   **核心意义**：GCP 代表了“安全线”。序列号小于等于 GCP 的所有操作，都已确保在所有活跃副本上持久化。如果主分片挂了，任何拥有 GCP 数据的副本都可以被选为新主，且保证数据不丢。

### 3.2 乐观并发控制 (Optimistic Concurrency Control)

在 6.0 之前，ES 使用 `_version` 字段进行乐观锁控制。现在，ES 推荐使用 `if_seq_no` 和 `if_primary_term` 参数。
*   **机制**：客户端读取文档获得 `seq_no=100, primary_term=5`。更新时带上这些参数。如果在此期间有另一个客户端更新了文档，SeqNo 变大，服务端校验发现不匹配，抛出 `VersionConflictEngineException`。
*   **优势**：这种机制在分布式系统中是严格全序的（Total Ordering），比简单的版本号更鲁棒，能够处理 Primary 切换后的复杂场景。

### 3.3 故障恢复机制 (Peer Recovery)

当一个节点重新加入集群，或者副本分片落后于主分片时，需要进行 **Peer Recovery**。

#### 3.3.1 挑战：Lucene 的不可变性

Lucene 的 Segment 是不可变的。删除文档实际上是在 `.del` 文件中打标记。但在后台 Merge 过程中，物理文件会被重写，已删除的文档会被彻底清除。这意味着如果一个副本离线时间超过了 Translog 的保留周期，且发生了 Merge，主分片就无法提供“历史操作”了。

#### 3.3.2 解决方案：Soft Deletes 与 Retention Leases

从 ES 7.0 开始，引入了 **Soft Deletes（软删除）** 机制。
*   **Soft Deletes**：在 Lucene 中，将“删除”操作实现为一种特殊的“更新”，保留墓碑（Tombstone）记录。这些记录在 Merge 时不会被立即清除。
*   **Retention Leases（保留租约）**：副本会向主分片注册一个租约，承诺“我会保留到 GCP 位置”。主分片会根据租约保留 Lucene 中的 Soft Deletes 历史记录，直到达到保留期限（默认 12 小时）。

#### 3.3.3 恢复策略对比

ES 会智能选择恢复策略：
1.  **基于操作的恢复 (Operation-based)**：**首选策略**。如果副本的 Retention Lease 仍然有效，且主分片保留了所有缺失的操作历史（在 Translog 或 Soft Deletes 中）。主分片只需将 `StartSeqNo` 到 `EndSeqNo` 之间的操作重放给副本。速度极快。
2.  **基于文件的恢复 (File-based)**：**兜底策略**。如果副本落后太多（历史操作已被 Merge 或 Translog 已轮转）。主分片必须将整个 Lucene 索引文件（Segments）通过网络拷贝给副本。这会导致巨大的网络开销和漫长的恢复时间。

---

## 4. 稳定性保障：资源管理与熔断器

ES 作为一个运行在 JVM 上的复杂分布式系统，内存管理是其稳定性的生命线。

### 4.1 熔断器体系 (Circuit Breakers)
熔断器的作用是在操作执行**之前**预估其内存消耗，如果超过阈值则直接拒绝请求，保护节点。

|**熔断器类型**|**监控对象**|**默认阈值**|**作用与建议**|
|---|---|---|---|
|**Parent Circuit Breaker**|所有熔断器的总和|JVM Heap 的 95%|最后的防线。如果触发，说明堆内存极其紧张，需扩容或优化查询|
|**Request Circuit Breaker**|单个请求的数据结构|JVM Heap 的 60%|防止复杂的聚合（如大基数 Terms 聚合）耗尽内存|
|**Fielddata Circuit Breaker**|加载到内存的 Text 字段|JVM Heap 的 40%|Text 字段默认无法聚合/排序，若强制开启 Fielddata，极易触发此熔断|
|**In-Flight Requests**|正在传输/处理中的请求|JVM Heap 的 100%|限制并发请求量，防止网络层积压过多数据|

### 4.2 内存管理最佳实践
*   **堆内存设置**：通常建议设置为物理内存的 50%，且不超过 32GB（以利用 Compressed OOPs 指针压缩优化）。剩余的 50% 留给操作系统的文件系统缓存（Filesystem Cache），这对于 Lucene 的段文件读取至关重要。
*   **Field Data 陷阱**：尽量避免在 `text` 字段上开启 `fielddata: true`。对于聚合和排序需求，应使用 `keyword` 类型，它使用磁盘上的 Doc Values 结构，对堆内存影响极小。

---

## 5. 理论高度：CAP 定理与 PACELC

将 Elasticsearch 置于分布式系统的理论框架中，可以更清晰地界定其适用场景与局限性。

### 5.1 CAP 定理中的定位
*   **Partition Tolerance (P)**：ES 严格处理网络分区。当脑裂发生时，ES 严格禁止少数派节点接受写入（甚至读取）。只有拥有过半数投票配置的子集群能继续工作。
*   **CP (Consistency)**：在写入路径上，ES 是 CP 系统。它牺牲了可用性（Availability）来换取数据的一致性。如果它允许双写，就会导致数据无法合并的冲突。
*   **AP (Availability)**：在读取路径上，ES 默认允许 `allow_partial_search_results: true`。这意味着如果部分分片不可用，搜索请求不会报错，而是返回剩下分片的结果。这是一种典型的 AP 行为。

### 5.2 PACELC 视角的权衡
PACELC 理论指出，即使没有网络分区（Else），系统也必须在延迟（Latency）和一致性（Consistency）之间做选择。
*   **写入路径**：倾向于 **Consistency**。为了数据安全，写入必须等待 Quorum 副本确认，这牺牲了写入延迟。
*   **搜索路径**：倾向于 **Latency**。搜索可以在任意副本上执行。由于 Refresh 是异步的（默认 1s），不同副本的数据可见性可能存在微小差异。这就是牺牲了一致性以换取极高的搜索并发能力和低延迟。

### 5.3 实时性悖论：Refresh 的代价
`refresh_interval` 是 ES 架构中最重要的权衡点之一。
*   **Search Real-time**：设为 1s。代价是产生大量微小 Segment，导致频繁 Merge，消耗 CPU 和 IO。
*   **Indexing Throughput**：设为 30s 或 -1。写入性能大幅提升，但数据可见性延迟增加。
*   **Wait For Refresh**：客户端可以使用 `?refresh=wait_for` 强制要求强一致性。但在高并发生产环境中，这会通过强制触发 Refresh 导致集群性能雪崩。

---

## 6. 总结

Elasticsearch 的宏观架构从早期的探索走向了如今的成熟与严谨。它通过引入 **TLA+ 验证的共识算法**，解决了分布式协同中的核心难题；通过 **PacificA 模型与 Sequence ID** 机制，构建了坚实的数据一致性底座；通过 **Translog 与 Retention Leases** 的精妙配合，实现了高效的故障恢复。

这一架构并非没有代价。近实时搜索的特性意味着在强一致性读上需要做额外工作。高可用性要求我们在存储成本（副本）和写入延迟（Quorum ACK）上做出让步。对于架构师而言，理解 `refresh_interval` 与 Segment Merge 的关系，以及 Voting Configuration 的工作方式，是设计真正容灾拓扑和调优集群性能的第一性原理。

### 附表：关键架构组件与分布式理论对应表

|**架构组件**|**对应的分布式理论/概念**|**作用描述**|**架构权衡 (Trade-off)**|
|---|---|---|---|
|**Voting Configuration**|Quorum (法定人数)|决定选主和状态提交的合法性|安全性 vs. 运维灵活性（不可同时下线过半）|
|**PacificA (Replication)**|Primary-Backup|保证主副分片数据同步|写入可用性 vs. 写入延迟|
|**Sequence ID (`_seq_no`)**|Total Ordering (全序)|操作定序，用于并发控制与恢复|增加元数据开销 vs. 解决乱序与冲突|
|**Global Checkpoint**|Watermark (水位线)|标识数据安全持久化位置|恢复速度 vs. 状态维护复杂性|
|**Translog**|Write-Ahead Log (WAL)|保证 NRT 架构下的数据持久性|写入 IOPS vs. 数据零丢失|
|**Soft Deletes**|Logical Deletion|支持基于操作的历史回溯|磁盘空间占用 vs. Peer Recovery 效率|
